Ana Estrada


The unknown target function is the neural net that correctly classifies all of the handwritten number samples. The training examples are previously classified images of handwritten numbers. There are about 60,000 samples that will be used for the training of the neural net. The other samples that will be taken from the keras database will be used as testing images for the testing of the hypothesis neural net. The hypothesis set is the set of all possible functions that can be generated from representation to map the images to their correct classification. The explanatory variable is the transformed images and the response variable is the classified images. The learning algorithm we are using is the multi-level perceptron learning model that is a feed forward neural network that uses stochastic gradient descent with a Relu activation function as the learning algorithm. The final hypothesis is the set of functions within the neural net that produced a 97% accuracy in classification of the testing set of data. 


The first thing that the model does is preprocessing the images into a format that is acceptable to the feed forward neural net. So the first thing the program does is load the data set into two tuples. One consisting of the training images and the correct classification of the images and the other consisting of the testing images and the correct classification of the images. After loading in the already gray-scaled images, then the images are reshaped in order for them to fit within the feed forward neural net model. Then you have to normalize the RGB values into values between 0 and 1 so that the weights are adjusted more easily by the learning algorithm. Then I set up the MLP classifier. I initialized it with two layers of 100 nodes and then tested it with 200 nodes so that I could ensure that I was surpassing the 95% accuracy threshold and this was a deep learning neural net not a representational learning neural net, but there was not much difference between running the model with 100 vs 200 hidden layers. The activation function that the model is using is the Relu activation function. Relu does not fall prey to the shortcomings of the sigmoid function in that it does not experience the vanishing gradient phenomenon, but instead because of its limits being 0 and infinity it is able to be a lot more flexible when optimizing the errors. This is also the standard activation function used for most neural nets. The learning rate is 0.01 which will tell us how much to let the gradient affect the weights and the verbose value was initialized to ten which basically just tells the model to print out the loss that each iteration experiences. We set the random state to 1 so that the weight matrix is initialized to meaningful small numbers that make it more easy for the model to adjust the weights. It is using a stochastic gradient descent function because it updates the weight incrementally as opposed to traditional gradient descent that adjusts the weights in batches. It also can escape the problem of reaching shallow local minima and instead is optimized to reach the global minimum. This makes the learning algorithm run faster than if we were using a more traditional gradient descent function. Then I fit the function using the training data that I had previously preprocessed. After the model is done being fit then I use the predict function to get the predicted values for the test set of data. Then in order to display the predictions of data both correct and incorrect I created a confusion matrix. This will basically tell you how many times the machine correctly predicted each number and how many times it classified a number as any number other than itself. I use the loss curve function to get the overage loss of my neural net and output that to the monitor in a graph. This graph shows the decrease in loss as the number of iterations went up. I display the training set score which basically tells the user the reported percentage of accuracy that the neural net experienced. I do the same thing for the test values. Now the neural net is ready to use with other images and will be approximately 97% accurate in its prediction of the numbers.

Sources the book, the slides, and Professor Watson