Ana Estrada


The unknown target function is the neural net that correctly classifies all of the handwritten number samples. The training examples are previously classified images of handwritten numbers. There are about 60,000 samples that will be used for the training of the neural net. The other samples that will be taken from the keras database will be used as testing images for the testing of the hypothesis neural net. The hypothesis set is the set of all possible functions that can be generated from representation to map the images to their correct classification. The explanatory variable is the transformed images and the response variable is the classified images. The learning algorithm we are using is the multi-level perceptron learning model that is a feed forward neural network that uses stochastic gradient descent with a sigmoid activation function as the learning algorithm. The initial training of the program consisted of 200 hidden nodes per hidden layer, a learning rate of 0.001 and 5 epochs. This run of the program resulted in a validation accuracy of 92%. The cost function we used was Mean Squared errors. After testing several different hyper parameter combinations the hyper parameters that I chose were 500 hidden nodes per layer, a learning rate of 0.01, and 10 epochs. This resulted in an 97% for testing data and a 98% accuracy for training data. 


I loaded the images from the keras database based on the instruction given in chapter 12 of the book. I used a helper function to unzip the files and then preprocess them into the correct format for the FFNN. The helper function then loads the data set into two tuples. One consisting of the training images and the correct classification of the images and the other consisting of the testing images and the correct classification of the images. After loading in the already gray-scaled images, then the images are reshaped in order for them to fit within the feed forward neural net model. Then you have to normalize the RGB values into values between 0 and 1 so that the weights are adjusted more easily by the learning algorithm. This is all done in the helper function called load_mnist. Then I called this function in my main and loaded in the training and test set of data. I preprocessed my data using a helper function that was called in the main function once. This allowed me to save the preprocessed data in a file so that the rest of the testing done on my model was not bogged down with the preprocessing of my data and the reloading of the zip function. So I only needed to call this with my initial run of the program. You will see the function call commented out.


Then I set up the neural network class. The initialization function takes in several parameters: the learning rate, the number of epochs, the number of nodes, and the minibatch size. We were chiefly concerned with the hyper parameters of the learning rate, number of epochs, and the number of nodes. I also take in a regularization parameter to reduce overfitting, but I tested it setting it to 0 so I don’t think it regularized anything. I also took in a mini batch variable that for stochastic gradient descent was set to zero. This allows the model to be tuned using these hyper parameters which was done after testing the initial hyperparameters that were mentioned above. The activation function that the model is using is the sigmoid activation function. This neural network class uses a sigmoid activation function so it sets up a sigmoid activation function that takes the raw signal output from the fit function and returns a value between 0 and 1 that will represent the probability that the sample belongs to a given class. The model will then use the highest probability to classify the handwritten number. The original learning rate was 0.001 which will tell us how much to let the gradient affect the weights.


After pulling in all of the necessary hyper parameters and getting them situated in the correct variable names I wrote the fit function. The fit function takes in the training and testing data for the model. This where the weights are going to be adjusted for the model. The weights are going to be adjusted using stochastic gradient descent which will be explained later on in this file. The first thing the fit function does is get the shape of the output which in this case is 10. Since there are 10 possible classification of the 10 single digit numbers in our dataset. Then it gets the number of features of the input. In this case the number of features is 784 which corresponds to the fact that the images that we are taking in are 28 by 28 pixels. I choose to initialize the weights here using a random normal distribution. This will give the weights small meaningful weights to begin adjusting the rest of the fit function. It is important to set them to small random numbers because it gives the program a place to start and it does not have to tune weights of zero. Also it is important that they are small because this will ensure that no one weight is dominating the model at the beginning of training. Then after this I one hot encode the training output so that it is one hot encoded with values of 0.1-0.999. This is important so the model can make accurate comparisons because it is giving us probabilities between 0 and 1.. Then the actual training begins. The model will iterate the training data based on the number of epochs provided in the initialization function. It will shuffle the data so that it is not in order that it was received to ensure a sort of even spread of training examples. Then because we are using stochastic gradient descent the learning algorithm will iterate over each individual example to calculate the forward pass and then the back propagation that will adjust the weight matrix. Forward propagation is done through the forward function. This function takes in the sample and applies the first hidden layers weight matrix. It then transforms the signal coming out of that layer using the sigmoid function and passes that signal to the second hidden layer where that same process will take place but instead this layer will pass its transformed signal to the output layer. The activation coming out of the output layer are the probabilities that the sample belongs to each class. It then returns the signals from each of these passes to the fit function to be used in stochastic gradient descent. 


The model is using a stochastic gradient descent function because it updates the weight incrementally as opposed to traditional gradient descent that adjusts the weights in batches. This allows the weights to be adjusted with every training example’s error as opposed to adjusting the weights with the error generated by the entire training set. This allows the program to run faster because it is adjusting the weights as we go through the model as opposed to running all the training examples and then calculating the error and adjusting the error. This also allows us to move around the cost function more easily because we are doing it sample by sample. It also can escape the problem of reaching shallow local minima and instead is optimized to reach the global minimum. This makes the learning algorithm run faster than if we were using a more traditional gradient descent function. Using stochastic gradient descent I back propagated the error through the model. To begin the back propagation I found the error between the output layer’s signal and the actual classification of that signal. This gave me the delta of the output layer or the error produced by the output later. Then I used this delta to calculate the delta of the second hidden layer or the error produced by the second hidden layer. To calculate the delta of the second hidden layer I used the delta of the output layer dot produced with the traverse of the output layer’s weight matrix and then multiplied that by the sigmoid derivative. You have to use the sigmoid derivative because you want to propagate back the raw untransformed signal to the previous layer. This ensures that the layer is calculating the correct error with respect to the raw output signal. This series of calculations gives the model the error that was produced by this hidden layer. I repeated this process with the first hidden layer and its corresponding values to calculate the error that was made in this layer of the model. Then using each of the deltas that I calculated for each layer, I found the gradient for each layer. I found the gradient by dot producting the amount of error or delta produced by the given layer with the signal that was propagated out of that layer. This is the partial derivative of the error function with respect to the output of each layer. This will give us the direction that we are going to move in the cost function. Finally I multiplied the change for each weight matrix by the learning rate and subtracted that from the weight matrix to get the new weight matrix. 


After executing back propagation for every sample for a given epoch. That was passed in in the training set I then evaluated my model with the testing data accuracy, training data accuracy, and cost. I used the predict function as what I believe you defined as a query. The predict function just basically takes in an array of samples and passes it through the forward pass function to get the prediction. It then returns the max of the untransformed signals which will be the classification of that sample. I return the predictions and use this to calculate accuracy and cost in the fit function. The cost function I used was mean squared errors. This cost function is effective because it allows us to capture the overall error in the model by using the changes squared so that negative and positive errors are not cancelling each other out. Then I displayed this in the terminal. I also created a dictionary to access the cost of each epoch so that I can graph it in the main function. I also stored the train accuracy and the testing accuracy. So that they could be potentially plotted in the main function. I defined query as an outward facing function that would take in a sample from the user and return the accuracy of the model with respect to the user given samples.


So the first thing that I did with my model is test it using the given initial parameters and the original training and testing data. I got around a 92% accuracy rate for my testing data. Then when I tested the different hyper parameters I found some interesting trends. When I tested six learning rates. It seemed that the larger the learning rate became the more error prone the model was. The two best performing learning rates were 0.01 and 0.001 and the worst performing learning rates were 0.2 and 0.4. The higher gradients were giving the model an exploding gradient and shooting the error up really high as can be seen in the second graph attached in the file. The smaller learning rates more gradually adjusted the weights by tempering the gradient in a more gradual manner as opposed to the higher learning rates. Which makes sense considering the graphs that we have been looking at in class and the techniques used to adjust the learning rate. It seems to be the case that a smaller learning rate tends to yield better results. Then I tested the number of hidden nodes. The optimal number of hidden nodes that I found were 600 nodes. Unsurprisingly the subsequent numbers of nodes each improved as they increased in the number of nodes. It doesn’t hurt to have more nodes like was stated in the stretchy pants strategy. With the number of epochs increasing you can see that the model improved with every increase in the number of epochs. Until the epochs grow greater then ten and then the performance degrades. I think you are getting problems that have to do with over fitting of the model. The next epochs are not adjusting the weights correctly.  When I ran the model with my ideal hyper parameters I got a 96% accuracy rate for my testing data. When I ran the model with my hand drawn images I got an accuracy rate of like 10%. I think that this is because my numbers might not have been centered like the original dataset. I was reading up on the mnist dataset and it says that the images are centered around the numbers. I definitely think mine were not centered. I think another problem with my data is maybe my numbers look really different from theirs also the machine did not have a lot of samples of my data so I think if i were to provide more I would get better accuracy. 


Finally I rotated my training data 10 degrees to the right and to the left. I then added them to my existing training set and trained the model again using these new images. I also was sure to add their respective labels into the labels of the training set. I was getting like a 98% accuracy rate for my testing data with these new images used for training. Training the model on the rotated images makes the model more generalizable. This model will perform better on samples that are off kilter. Also this is a good way to reuse data and not have to collect anymore new data. These transformations allow us to create a more accurate model and not have to collect anymore data.


Sources the book, the slides, and Professor Watson, discussed pseudo code with Yosef (mostly the rotating of the images at the end), i also used this website to teach me how to take in images.